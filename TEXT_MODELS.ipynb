{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPiYUUkBrdSK"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "import matplotlib.pyplot as plt # data visualisation\n",
        "import seaborn as sns  # data visualisation\n",
        "%matplotlib inline\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CBB = pd.read_csv(\"CollegeBasketballPlayers2009-2021.csv\")\n",
        "NBA = pd.read_csv(\"ALL YEARS 2009 - 2022.csv\")"
      ],
      "metadata": {
        "id": "OpIrdVzBr1tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cfe9706-543f-42b9-b0c8-10795166087d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-063c86202999>:1: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  CBB = pd.read_csv(\"CollegeBasketballPlayers2009-2021.csv\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#turnover / (turover + assist)\n",
        "CBB = CBB.dropna()\n",
        "# Replace 0s in ast/tov column with a small positive number\n",
        "CBB.loc[CBB['ast/tov'] == 0, 'ast/tov'] = 0.0001\n",
        "\n",
        "# Calculate the new column and add it to the DataFrame\n",
        "CBB['PR'] = 1 / CBB['ast/tov']\n",
        "\n",
        "# Save the modified DataFrame to a new CSV file\n",
        "CBB.to_csv('CollegeBasketballPlayers2009-2021_with_ratio.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ndp18BEz2Db",
        "outputId": "f420ebc3-c2f5-4aa5-da72-7c319ad74ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ca63aac08c17>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  CBB['PR'] = 1 / CBB['ast/tov']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subset_model_all_features(college_file, nba_file, features, k=9, test_year=2020, test_size=0.2, random_state=42, new_data=None):\n",
        "    # Load the data from the CSV files\n",
        "    college_data = pd.read_csv(college_file)\n",
        "    nba_data = pd.read_csv(nba_file)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for feat in features:\n",
        "        # Find the latest year in college_data that is before the test_year\n",
        "        year = college_data['year'].max()\n",
        "        while year >= test_year:\n",
        "            year -= 1\n",
        "\n",
        "        # Merge college_data with nba_data to get target variable (PR)\n",
        "        merged_data = pd.merge(college_data[college_data['year'] == year], nba_data[['PLAYER', feat]], left_on='player_name', right_on='PLAYER')\n",
        "\n",
        "        # Drop any rows with missing target variable values\n",
        "        merged_data = merged_data.dropna(subset=[feat])\n",
        "\n",
        "        # Split the data into features (X) and target (y)\n",
        "        # Modify this line to drop only columns not relevant for training\n",
        "        X = merged_data.drop(['player_name', 'PLAYER', feat, 'team', 'conf', 'Unnamed: 64', 'Unnamed: 65', 'yr', 'ht', 'type', 'year'], axis=1)\n",
        "        y = merged_data[feat]\n",
        "\n",
        "        # The rest of your function can remain the same\n",
        "        # Use feature selection to identify the most important features\n",
        "        selector = SelectKBest(score_func=f_regression, k=k)\n",
        "\n",
        "        # Impute missing values using mean strategy\n",
        "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "        # Create a pipeline to handle feature selection and imputation\n",
        "        pipeline = Pipeline([('imputer', imputer), ('selector', selector)])\n",
        "\n",
        "        # Fit the pipeline on the data and transform it\n",
        "        X_selected = pipeline.fit_transform(X, y)\n",
        "\n",
        "        # Get the indices and names of the most important features\n",
        "        indices = selector.get_support(indices=True)\n",
        "        feature_names = X.columns[indices]\n",
        "\n",
        "        # Split the subset of data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "        # Train a machine learning model on the subset of data\n",
        "        model = RandomForestRegressor(random_state=random_state)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the testing set\n",
        "        score = model.score(X_test, y_test)\n",
        "\n",
        "        # Make predictions on new data using the subset of features, if provided\n",
        "        if new_data is not None:\n",
        "            X_new = pd.DataFrame(new_data, columns=feature_names)\n",
        "            X_new_selected = pipeline.transform(X_new)\n",
        "            predictions = model.predict(X_new_selected)\n",
        "            results[feat] = {'score': score, 'selected_features': feature_names, 'predictions': predictions}\n",
        "        else:\n",
        "            results[feat] = {'score': score, 'selected_features': feature_names}\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "IQvsucCKT4U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['FG%', 'PR', '3P%', 'PF', 'PTS']\n",
        "results = subset_model_all_features(\"CollegeBasketballPlayers2009-2021.csv\", \"ALL YEARS 2009 - 2022.csv\", features, k=9)\n",
        "\n",
        "for feat, data in results.items():\n",
        "    print(f\"{feat} prediction score: {data['score']}\")\n",
        "    print(f\"Selected features for {feat} prediction: {', '.join(data['selected_features'])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ynpo1p0FT4ne",
        "outputId": "84b0b57a-1e85-4d5e-8a68-68c807d26868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-78-d4eb9e5ec20e>:3: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  college_data = pd.read_csv(college_file)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FG% prediction score: 0.4782981676253756\n",
            "Selected features for FG% prediction: ORB_per, TPM, TPA, blk_per, rimmade/(rimmade+rimmiss), dunksmade, dunksmiss+dunksmade, oreb, blk\n",
            "PR prediction score: 0.35461435643564254\n",
            "Selected features for PR prediction: GP, DRB_per, AST_per, ast/tov, dunksmiss+dunksmade, drtg, adrtg, dbpm, ast\n",
            "3P% prediction score: 0.05779123334932568\n",
            "Selected features for 3P% prediction: DRB_per, TO_per, FT_per, TPM, TPA, pfr, rimmade/(rimmade+rimmiss), dunksmade, dunksmiss+dunksmade\n",
            "PF prediction score: -0.9945365177195682\n",
            "Selected features for PF prediction: AST_per, blk_per, pfr, Rec Rank, dunksmade, dunksmiss+dunksmade, drtg, adrtg, dbpm\n",
            "PTS prediction score: 0.13788307295723412\n",
            "Selected features for PTS prediction: Ortg, porpag, adjoe, rimmade, pick, bpm, obpm, gbpm, ogbpm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def neural_network_model(csv_file, nba_file, features, test_year=2020, test_size=0.2, random_state=42):\n",
        "    # Load the data from the CSV files\n",
        "    college_data = pd.read_csv(csv_file, low_memory=False)\n",
        "    nba_data = pd.read_csv(nba_file)\n",
        "\n",
        "    # Initialize an empty dictionary to store R-squared scores\n",
        "    scores = {}\n",
        "\n",
        "    for feat in features:\n",
        "        # Find the latest year in college_data that is before the test_year\n",
        "        year = college_data['year'].max()\n",
        "        while year >= test_year:\n",
        "            year -= 1\n",
        "\n",
        "        # Merge college_data with nba_data to get target variables (PR and PTS)\n",
        "        merged_data = pd.merge(college_data[college_data['year'] <= year], nba_data[['PLAYER', feat, 'PTS']], left_on='player_name', right_on='PLAYER')\n",
        "\n",
        "        # Drop any rows with missing target variable values\n",
        "        merged_data = merged_data.dropna(subset=['PTS', feat])\n",
        "\n",
        "        # Split the data into features (X) and target (y)\n",
        "        X = merged_data.drop(['player_name', 'PLAYER', 'team', 'PTS', feat, 'yr', 'conf', 'Unnamed: 64', 'Unnamed: 65', 'year', 'ht', 'type'], axis=1)\n",
        "        y = merged_data[feat]\n",
        "\n",
        "        # Impute missing values using mean strategy\n",
        "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "        X = imputer.fit_transform(X)\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "        # Train a neural network model on the training set\n",
        "        model = MLPRegressor(hidden_layer_sizes=(100,100), max_iter=100, random_state=random_state)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the testing set\n",
        "        score = model.score(X_test, y_test)\n",
        "\n",
        "        # Store the R-squared score for this feature in the scores dictionary\n",
        "        scores[feat] = score\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "M89VNLa_EnQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['FG%', 'PR', '3P%', 'PF', 'PTS']\n",
        "scores = neural_network_model(\"CollegeBasketballPlayers2009-2021.csv\", \"ALL YEARS 2009 - 2022.csv\", features, test_year=2020)\n",
        "\n",
        "for feat in features:\n",
        "    print(f\"R^2 score for {feat} prediction: {scores[feat]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvGdA0R1FvwE",
        "outputId": "0c279bea-f3dd-4bef-c757-c14d2ccafb67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R^2 score for FG% prediction: -0.522984626690858\n",
            "R^2 score for PR prediction: -1.2076107263271623\n",
            "R^2 score for 3P% prediction: 0.27430483863988964\n",
            "R^2 score for PF prediction: -0.09391018121250139\n",
            "R^2 score for PTS prediction: 0.03135174018700265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "def neural_network_model_classification(csv_file, nba_file, features, test_year, test_size=0.2, random_state=42):\n",
        "    # Load the data from the CSV files\n",
        "    college_data = pd.read_csv(csv_file, low_memory=False)\n",
        "    nba_data = pd.read_csv(nba_file)\n",
        "\n",
        "    # Initialize an empty dictionary to store accuracy scores\n",
        "    scores = {}\n",
        "\n",
        "    for feat in features:\n",
        "        # Find the latest year in college_data that is before the test_year\n",
        "        year = college_data[college_data['year'] < test_year]['year'].max()\n",
        "\n",
        "        # Merge college_data with nba_data to get target variables (above/below median)\n",
        "        merged_data = pd.merge(college_data[college_data['year'] == year], nba_data[['PLAYER', feat]], left_on='player_name', right_on='PLAYER')\n",
        "\n",
        "        # Drop any rows with missing target variable values\n",
        "        merged_data = merged_data.dropna(subset=[feat])\n",
        "\n",
        "        # Calculate the median of the target variable\n",
        "        target_median = merged_data[feat].median()\n",
        "\n",
        "        # Classify the samples as above or below median\n",
        "        merged_data['target'] = merged_data[feat].apply(lambda x: 1 if x >= target_median else 0)\n",
        "\n",
        "        # Split the data into features (X) and target (y)\n",
        "        X = merged_data.drop(['player_name', 'PLAYER', 'team', 'target', 'conf', 'Unnamed: 64', 'Unnamed: 65', 'year', 'yr', 'ht', 'type', feat], axis=1)\n",
        "        y = merged_data['target']\n",
        "\n",
        "        # Impute missing values using mean strategy\n",
        "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "        X = imputer.fit_transform(X)\n",
        "\n",
        "        # Standardize the features\n",
        "        scaler = StandardScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "        # Train a neural network model on the training set\n",
        "        model = MLPClassifier(hidden_layer_sizes=(100,100), max_iter=100, random_state=random_state)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model on the testing set\n",
        "        score = model.score(X_test, y_test)\n",
        "\n",
        "        # Store the accuracy score for this feature in the scores dictionary\n",
        "        scores[feat] = score\n",
        "\n",
        "    return scores\n"
      ],
      "metadata": {
        "id": "VDyfaltHbYd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['FG%', 'PR', '3P%', 'PF', 'PTS']\n",
        "score = neural_network_model_classification(\"CollegeBasketballPlayers2009-2021.csv\", \"ALL YEARS 2009 - 2022.csv\", features, test_year=2020)\n",
        "\n",
        "print(f\"Accuracy score for classification using data from 2009-2019 to predict 2019-2020: {score}\")\n"
      ],
      "metadata": {
        "id": "BD3Y_W8qbYos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c32f28c8-568c-44e3-f9a2-fd48f474384a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score for classification using data from 2009-2019 to predict 2019-2020: {'FG%': 0.6153846153846154, 'PR': 0.5384615384615384, '3P%': 0.5384615384615384, 'PF': 0.6923076923076923, 'PTS': 0.46153846153846156}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}